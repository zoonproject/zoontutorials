<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Model Evaluation • zoontutorials</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">zoontutorials</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/Choosing_A_Modelling_Method.html">Choosing A Modelling Method</a>
    </li>
    <li>
      <a href="../articles/Data_Exploration.html">Data exploration</a>
    </li>
    <li>
      <a href="../articles/Data_Sources.html">Data Sources</a>
    </li>
    <li>
      <a href="../articles/Introduction.html">Introduction to SDMs</a>
    </li>
    <li>
      <a href="../articles/Model_Validation.html">Model Evaluation</a>
    </li>
    <li>
      <a href="../articles/Reproducibility.html">Reproducibility</a>
    </li>
    <li>
      <a href="../articles/Selecting_Covariates.html">Selecting and preparing covariates</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>Model Evaluation</h1>
            
          </div>

    
    
<div class="contents">
<hr>
<div id="introduction" class="section level1">
<h1 class="hasAnchor">
<a href="#introduction" class="anchor"></a>Introduction</h1>
<p>Species distribution models are used for all sorts of important decisions: choosing protected areas, prioritising conservation actions, informing development applications, even determining the conservation status of species. As such, we need to know how much faith we can put in our models. We need to know if they are up to the task we set for them. We do that by evaluating our model’s performance.</p>
<p>Model evaluation is the process of determining if our model is close enough to reality for our purpose. Models are smaller, simpler, hopefully useful versions of reality, but all models are wrong. Whether our imperfect model is useful is dependent on what we’re going to use it for. For example, if we wish to predict the geographic expansion of a species under a changing climate, we need to know if our model can extrapolate across geographic space. But, if we wish only to accurately map the current distribution of a species, we don’t mind much at all if our model can extrapolate accurately. It is thus important to consider not how good our model is in absolute terms, but rather if our model is <em>good enough</em> for its purpose. Model evaluation, then, is the process of checking our model outputs to determine if they are a close enough representation of reality for the purpose you’re using it for.</p>
<div id="fit-for-purpose" class="section level2">
<h2 class="hasAnchor">
<a href="#fit-for-purpose" class="anchor"></a>Fit-for-purpose</h2>
<ul>
<li>tradeoff of fit and predictive capacity</li>
</ul>
<p>The purpose of our model will decide for us both how we evaluate our model and whether the goodness of fit, or lack thereof, determined by our evaluation is tolerable. Determinig if our model is ‘good enough’ is not a simple task and generally has to be justified on a case by case basis. Are we building a model to identify gradients species respond to and contribute to ecological knowledge? Or maybe we are running models on behalf of a governmental agency and thus require accurate models? We’ll address some of these nuances throughout this guide. However, there is one facet of model fit worht discussing straight up - over-fitting.</p>
<p>Models are considered over-fitted when they go beyond identifying a pattern in data and instead fit a relationship to the noise in the data - to the stochastic variation. In species distribution models, sometimes we want to avoid over-fitting and sometimes we don’t mind. Generally, if we only want to know about here and now as precisely as possible, over-fitting our models is of littel concern. For example, if there is a location where a species is found but that location is an outlier as far as the species is concerned, we still want to protect it. Thus, a model which estimates that location as a presence if useful. If we use a model that determines that site is an outlier, and therefore has a low probability of presence, we will miss it in our conservation prioritisations. Over-fitted models are difficult to identify and the jury is still out on precisley what makes a model over-fitted.</p>
<p>Model evaluation then, even over-fitted models, is an exercise in justifying that our model is fit for the purpose for which we made it. It is not an exercise in validating that our models perfectly reflect reality. For example, a model that only explains 15% of the variance in the data but identifies an important environmental gradient is still a useful model for the purpose of generating ecological knowledge. It is probably not a model we should use to inform development applications but it is still a useful model.</p>
<div id="calibration-versus-discrimination" class="section level3">
<h3 class="hasAnchor">
<a href="#calibration-versus-discrimination" class="anchor"></a>Calibration versus discrimination</h3>
</div>
<div id="we-cannot-validate-a-model-by-comparing-it-to-another-model" class="section level3">
<h3 class="hasAnchor">
<a href="#we-cannot-validate-a-model-by-comparing-it-to-another-model" class="anchor"></a>We cannot validate a model by comparing it to another model</h3>
</div>
</div>
<div id="how" class="section level2">
<h2 class="hasAnchor">
<a href="#how" class="anchor"></a>How</h2>
<p>In this <code>zoon</code> guide, we will discuss the different methods of evaluating a model (or set of models), when we might choose one method of evaluation over another, and how to use <code>zoon</code> to implement these different evaluation methods. In this guide, we split model evaluation methods and metrics into three broad categories increasing in thoroughness: bare minimum, internal evaluation, and cross-validation.</p>
<p>However, no matter which category of evaluation method and metric we choose, there are a few over-arhcing princples to keep in mind. Firstly, we want our results to <em>make sense</em> ecologically and reflect our raw data. Secondly, we would like the most parsimoneous model. Some old white man said ‘the simplest answer is usualy the right answer’. Thus, if given the choice between two equally performing models, we would choose the least complicated model, the model with the fewest environmental covariates. Lastly, we want our model to be able to predict the data we fitted it with; we want it to be accurate and not biased. In this guide, we’ll discuss the various model evaluation methods and metrics in line with these guiding principles.</p>
<div id="the-bare-minimum" class="section level3">
<h3 class="hasAnchor">
<a href="#the-bare-minimum" class="anchor"></a>The bare minimum</h3>
<p>Let’s first run through the Bare Minimum. The Bare Minimum is what we absolutely must do in order to be able to defend our choice of model. Remember a model consists of the response data, the environmental covariates, and the modelling method we choose. Some of the bare mimimum is just part and parcel of interpretting the results, but while we’re interpretting our results, it’s good to remember that we need to question the model not accept the printed map or response curve as gospel truth.</p>
<p>The bare minimum can be split into two categories: checking the model results against our data and checking the model against our knowledge of the ecology we’re modelling.</p>
<p>Check maps match data (interactive map)</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(zoon)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">zoon_workflow &lt;-<span class="st"> </span><span class="kw"><a href="http://www.rdocumentation.org/packages/zoon/topics/workflow">workflow</a></span>(<span class="dt">occurrence =</span> CarolinaWrenPA,
                          <span class="dt">covariate =</span> CarolinaWrenRasters,
                          <span class="dt">process =</span> NoProcess,
                          <span class="dt">model =</span> LogisticRegression,
                          <span class="dt">output =</span> InteractiveMap)</code></pre></div>
<p>One can go a little bit further in evaluating model fit by plotting species repsonse curves where available (response curves)</p>
<p>For parametric models, check coefficients make sense (go the same way as data, giev back sensible results) (coefficient plot)</p>
</div>
</div>
</div>
<div id="independence" class="section level1">
<h1 class="hasAnchor">
<a href="#independence" class="anchor"></a>Independence</h1>
<p>An underlying assumption of most presence-background regression models is that the occurrence data are conditionally independent given the covariates, meaning the covariates are expected to explain all the spatial variation in our data. Typically, however, this is not the case. Spatial dependence, for example through clustering or social aggregation of species, may take place, invalidating this assumption of independence. There are a handful of modelling approaches to identifying spatial dependence structures, including Gibbs or Cox process models, that will be discussed elsewhere.</p>
<div id="internal-evalution" class="section level3">
<h3 class="hasAnchor">
<a href="#internal-evalution" class="anchor"></a>Internal evalution</h3>
<p>The next step up in the model evaluation game is internal evaluation. Internal evaluation checks if our model is a good description of the data we fit it with.</p>
<div id="deviance-and-variance" class="section level4">
<h4 class="hasAnchor">
<a href="#deviance-and-variance" class="anchor"></a>Deviance and Variance</h4>
<p>R-squared (variance explained)</p>
<p>Deviance</p>
</div>
<div id="information-criteria" class="section level4">
<h4 class="hasAnchor">
<a href="#information-criteria" class="anchor"></a>Information criteria</h4>
<p>Adding more parameters almost always improves the fit of the model, so why not just put them all in? While adding more parameters improves the fit, it may reduce the ability of the model to predict new data outside your current sample.</p>
<p>There are two fundamental types of statistical error that we must deal with when evaluating and interpreting model results: overfitting and underfitting. Both of these errors lead to poor prediction by the model, or its ability to retrodict the original data. When we overfit our data, we learn too much from our data, while underfitting leads us to learn too little from our data.</p>
<p>The information theoretic approach to model selection uses information criteria, which are metrics to score models by prediction accuracy with a penalty for complexity.</p>
<p>This approach is in contrast to significance testing approaches that focus on selecting models where all terms have &lt;0.05 significance. Some ‘significant’ terms may not help improve prediction, whereas some that are not might.</p>
<p>Information criteria build on deviance measures by penalising models for additional covariates. That is, they select for accurate and <em>parimsoneous</em> models.</p>
</div>
</div>
<div id="cross-validation" class="section level3">
<h3 class="hasAnchor">
<a href="#cross-validation" class="anchor"></a>Cross-validation</h3>
<p>Model cross-validation is the gold standard of model evalution in species distritbuion models. In the absence of a independent dataset, that is.</p>
<p>There’s many way to do cross-validation and again it depends on what aspect of model performance we want to check.</p>
<p>Cross-validation works as follows - you iteratively split the data into a test and training dataset. The model is fit (or ‘trained’) with the traiing dataset and evaluated with the test dataset.</p>
<div id="k-fold" class="section level4">
<h4 class="hasAnchor">
<a href="#k-fold" class="anchor"></a>K-fold</h4>
</div>
<div id="bootstrap" class="section level4">
<h4 class="hasAnchor">
<a href="#bootstrap" class="anchor"></a>Bootstrap</h4>
</div>
<div id="extrapolation-in-space" class="section level4">
<h4 class="hasAnchor">
<a href="#extrapolation-in-space" class="anchor"></a>Extrapolation in space</h4>
<p>Want to do if need to predict range shift or to a new location. Olden (ref) refer to this as the transferability of the model in space. And you do it by</p>
<p>Can your model predict extremes? Can it extrpolate.</p>
</div>
<div id="the-controversial-auc" class="section level4">
<h4 class="hasAnchor">
<a href="#the-controversial-auc" class="anchor"></a>The controversial AUC</h4>
</div>
<div id="specificity" class="section level4">
<h4 class="hasAnchor">
<a href="#specificity" class="anchor"></a>Specificity</h4>
</div>
<div id="selectivity" class="section level4">
<h4 class="hasAnchor">
<a href="#selectivity" class="anchor"></a>Selectivity</h4>
</div>
</div>
<div id="call-to-action-snappy-sub-title" class="section level2">
<h2 class="hasAnchor">
<a href="#call-to-action-snappy-sub-title" class="anchor"></a>Call to action, snappy sub-title</h2>
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li>
<a href="#introduction">Introduction</a><ul class="nav nav-pills nav-stacked">
<li><a href="#fit-for-purpose">Fit-for-purpose</a></li>
      <li><a href="#how">How</a></li>
      </ul>
</li>
      <li>
<a href="#independence">Independence</a><ul class="nav nav-pills nav-stacked">
<li><a href="#call-to-action-snappy-sub-title">Call to action, snappy sub-title</a></li>
      </ul>
</li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Nick Golding, David Wilkinson, Liz Martin, Saras Windecker.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  </body>
</html>
